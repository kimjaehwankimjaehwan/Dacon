{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjaehwankimjaehwan/Dacon/blob/main/seyonec_PubChem10M_SMILES_BPE_450k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ITCrlDVq9PSV",
        "outputId": "40d1e7a2-c5ae-4655-8acd-9065e6f8e06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch rdkit pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWuoil99SN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "28c92596-3424-4a78-b505-1363b313db6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [294/294 01:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.653618</td>\n",
              "      <td>0.808466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.471863</td>\n",
              "      <td>0.686923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.480935</td>\n",
              "      <td>0.693495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 0.6869229707557296\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ì„¤ì •\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'MODEL_NAME': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
        "    'BATCH_SIZE': 16,\n",
        "    'EPOCHS': 3,\n",
        "    'LR': 5e-5,\n",
        "}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "chembl_data = pd.read_csv('train.csv')  # ì˜ˆì‹œ íŒŒì¼ ì´ë¦„\n",
        "train, val = train_test_split(chembl_data, test_size=0.2, random_state=CFG['SEED'])\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG['MODEL_NAME'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(CFG['MODEL_NAME'], num_labels=1)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì •ì˜\n",
        "class SMILESDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128, has_target=True): # Added has_target parameter\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_target = has_target # Store has_target value\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data.iloc[index]['Smiles']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.has_target: # Check if target should be included\n",
        "            target = self.data.iloc[index]['pIC50']\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(target, dtype=torch.float)\n",
        "            }\n",
        "        else: # Return only input_ids and attention_mask\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            }\n",
        "\n",
        "train_dataset = SMILESDataset(train, tokenizer)\n",
        "val_dataset = SMILESDataset(val, tokenizer)\n",
        "\n",
        "# TrainingArguments ë° Trainer ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch', #Evaluation strategy matches save strategy\n",
        "    learning_rate=CFG['LR'],\n",
        "    per_device_train_batch_size=CFG['BATCH_SIZE'],\n",
        "    per_device_eval_batch_size=CFG['BATCH_SIZE'],\n",
        "    num_train_epochs=CFG['EPOCHS'],\n",
        "    seed=CFG['SEED'],\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    save_strategy = 'epoch' # Changed to epoch to match evaluation strategy\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {'rmse': np.sqrt(mean_squared_error(p.label_ids, p.predictions.flatten()))}\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ\n",
        "trainer.train()\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
        "val_preds = trainer.predict(val_dataset)\n",
        "val_rmse = np.sqrt(mean_squared_error(val['pIC50'], val_preds.predictions.flatten()))\n",
        "print(f'Validation RMSE: {val_rmse}')\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_dataset = SMILESDataset(test, tokenizer, has_target=False) # Set has_target to False for the test dataset\n",
        "test_preds = trainer.predict(test_dataset)\n",
        "\n",
        "# pIC50ì„ IC50ìœ¼ë¡œ ë³€í™˜\n",
        "def pIC50_to_IC50(pic50_values):\n",
        "    return 10 ** (9 - pic50_values)\n",
        "\n",
        "test['IC50_nM'] = pIC50_to_IC50(test_preds.predictions.flatten())\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['IC50_nM'] = test['IC50_nM']\n",
        "submit.to_csv('./transformer_baseline_submit.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0t2vd659SRY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "76a141ac-b1cc-4355-dd25-bc3462ba0231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [490/490 01:53, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.698729</td>\n",
              "      <td>0.835900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.521701</td>\n",
              "      <td>0.722289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.624996</td>\n",
              "      <td>0.790567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.534792</td>\n",
              "      <td>0.731295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.556339</td>\n",
              "      <td>0.745881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 0.7222886035452358\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ì„¤ì •\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'MODEL_NAME': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
        "    'BATCH_SIZE': 8,  # Batch size reduced for finer gradients\n",
        "    'EPOCHS': 5,  # Increased number of epochs\n",
        "    'LR': 2e-5,  # Reduced learning rate for more stable training\n",
        "    'WARMUP_RATIO': 0.1,  # Warmup ratio for the learning rate scheduler\n",
        "}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "chembl_data = pd.read_csv('train.csv')  # ì˜ˆì‹œ íŒŒì¼ ì´ë¦„\n",
        "train, val = train_test_split(chembl_data, test_size=0.2, random_state=CFG['SEED'])\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG['MODEL_NAME'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(CFG['MODEL_NAME'], num_labels=1)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì •ì˜\n",
        "class SMILESDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128, has_target=True):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_target = has_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data.iloc[index]['Smiles']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.has_target:\n",
        "            target = self.data.iloc[index]['pIC50']\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(target, dtype=torch.float)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            }\n",
        "\n",
        "train_dataset = SMILESDataset(train, tokenizer)\n",
        "val_dataset = SMILESDataset(val, tokenizer)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LR'])\n",
        "total_steps = len(train_dataset) // CFG['BATCH_SIZE'] * CFG['EPOCHS']\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(CFG['WARMUP_RATIO'] * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# TrainingArguments ë° Trainer ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=CFG['LR'],\n",
        "    per_device_train_batch_size=CFG['BATCH_SIZE'],\n",
        "    per_device_eval_batch_size=CFG['BATCH_SIZE'],\n",
        "    num_train_epochs=CFG['EPOCHS'],\n",
        "    seed=CFG['SEED'],\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    save_strategy='epoch',\n",
        "    gradient_accumulation_steps=2,  # To simulate a larger batch size\n",
        "    #optimizers=(optimizer, scheduler)  # Custom optimizer and scheduler - This line is removed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {'rmse': np.sqrt(mean_squared_error(p.label_ids, p.predictions.flatten()))}\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ\n",
        "trainer.train()\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
        "val_preds = trainer.predict(val_dataset)\n",
        "val_rmse = np.sqrt(mean_squared_error(val['pIC50'], val_preds.predictions.flatten()))\n",
        "print(f'Validation RMSE: {val_rmse}')\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_dataset = SMILESDataset(test, tokenizer, has_target=False)\n",
        "test_preds = trainer.predict(test_dataset)\n",
        "\n",
        "# pIC50ì„ IC50ìœ¼ë¡œ ë³€í™˜\n",
        "def pIC50_to_IC50(pic50_values):\n",
        "    return 10 ** (9 - pic50_values)\n",
        "\n",
        "test['IC50_nM'] = pIC50_to_IC50(test_preds.predictions.flatten())\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['IC50_nM'] = test['IC50_nM']\n",
        "submit.to_csv('./transformer_tuned_submit.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. í•™ìŠµë¥ (LR): ê¸°ë³¸ í•™ìŠµë¥ ì„ 2e-5ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë” ë‚®ì€ í•™ìŠµë¥ ì€ í•™ìŠµì˜ ì•ˆì •ì„±ì„ ë†’ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "2. ì—í¬í¬ ìˆ˜ ì¦ê°€: ì—í¬í¬ ìˆ˜ë¥¼ 5ë¡œ ëŠ˜ë ¤ì„œ ëª¨ë¸ì´ ì¶©ë¶„íˆ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.\n",
        "3. ë°°ì¹˜ í¬ê¸° ê°ì†Œ: ë°°ì¹˜ í¬ê¸°ë¥¼ 8ë¡œ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ê³ , ì„¸ë°€í•œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.\n",
        "4. Warmup ë‹¨ê³„ ì¶”ê°€: WARMUP_RATIOë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° ëª‡ ë‹¨ê³„ ë™ì•ˆ í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” warmup ë‹¨ê³„ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n",
        "5. Gradient Accumulation: ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , 6. gradient_accumulation_stepsë¥¼ 2ë¡œ ì„¤ì •í•˜ì—¬ ì‹¤ì§ˆì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ 16ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í–ˆìŠµë‹ˆë‹¤.\n",
        "6. Custom Optimizer and Scheduler: ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì§ì ‘ ì„¤ì •í•˜ì—¬ ë” ì •ë°€í•˜ê²Œ í•™ìŠµì„ ì œì–´í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "eDjdJ6APAbv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hENqrBjW9SUe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvXMd0Uo9SY-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTm-qBFq9Sav"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPaeTAU3G0noOCu4MdXOyHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}